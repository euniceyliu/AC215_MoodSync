# LLM-RAG

## Run Docker
1. First run `sh docker-shell.sh` to build the containers.
2. Then, run `docker compose up` to ensure the connection between the chromadb container and the llm-rag container over the llm-rag-network network. 
3. Once inside the app, run `python llm_rag.py --load` to load the preprocessed RAG embeddings into the chromadb database.
4. Now, you can run `python llm_rag.py --chat` to see the response generated by the foundation model with the most relevant RAG information from chromadb being provided to the model. Notice that the prompt is pre-set and you modify it in the llm_ray.py script, specifically inside the chat() function.
