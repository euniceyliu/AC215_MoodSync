# LLM-RAG

## Run Docker
1. First run `sh docker-shell.sh` to build the containers.
2. Then, `exit` out of the running container, run `docker compose up` to ensure the connection between the chromadb container and the llm-rag container over the llm-rag-network network. Use ctrl+c to exit.
3. Run `sh docker-shell.sh` again, and now the docker should be running two containers.

## Run Container and llm-rag
4. Once inside the app, run `python llm_rag.py --load` to load the preprocessed RAG embeddings into the chromadb database.
5. Now, you can run `python llm_rag.py --chat` to see the response generated by the foundation model with the most relevant RAG information from chromadb being provided to the model. Notice that the prompt is pre-set and you modify it in the llm_ray.py script, specifically inside the chat() function.
